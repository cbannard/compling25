{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ada7675f",
      "metadata": {
        "id": "ada7675f"
      },
      "source": [
        "# LELA32052 Computational Linguistics Week 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ede3dfeb",
      "metadata": {
        "id": "ede3dfeb"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f508cffb",
      "metadata": {
        "id": "f508cffb"
      },
      "source": [
        "### Escaping special characters\n",
        "We have learned about a number of characters that have a special meaning in regular expressions (periods, dollar signs etc). We might sometimes want to search for these characters in strings. To do this we can \"escape\" the character using a double backslash() as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02871a24",
      "metadata": {
        "id": "02871a24"
      },
      "outputs": [],
      "source": [
        "opening_sentence = \"On an exceptionally hot evening early in July a young man came out of the garret in which he lodged in S. Place and walked slowly, as though in hesitation, towards K. bridge.\"\n",
        "re.findall(\"\\\\.\",opening_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f53a7a48",
      "metadata": {
        "id": "f53a7a48"
      },
      "source": [
        "### re.split()\n",
        "In week 1 we learned to tokenise a string using the string function split. re also has a split function. re.split() takes a regular expression as a first argument (unless you have a precompiled pattern) and a string as second argument, and split the string into tokens divided by all substrings matched by the regular expression.\n",
        "\n",
        "\n",
        "**Problem 1: Can you improve on the following tokeniser?**\n",
        "\n",
        "In doing so you might need to extend your knowledge of regular expressions and employ one of the special characters included here: https://www.dataquest.io/wp-content/uploads/2019/03/python-regular-expressions-cheat-sheet.pdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5543015b",
      "metadata": {
        "id": "5543015b"
      },
      "outputs": [],
      "source": [
        "to_split_on_word = re.compile(\" \")\n",
        "opening_sentence_new = to_split_on_word.split(opening_sentence)\n",
        "print(opening_sentence_new)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cb5bc5b",
      "metadata": {
        "id": "9cb5bc5b"
      },
      "source": [
        "# Sentence Segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48df58a8",
      "metadata": {
        "id": "48df58a8"
      },
      "source": [
        "Above we split a sentence into words. However most texts that we want to process have more than one sentence, so we also need to segment text into sentences. We will work with the first chapter of Crime and Punishment again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a1dd2b7",
      "metadata": {
        "id": "4a1dd2b7"
      },
      "outputs": [],
      "source": [
        "from io import RawIOBase\n",
        "!wget https://www.gutenberg.org/files/2554/2554-0.txt\n",
        "f = open('2554-0.txt')\n",
        "raw= f.read()\n",
        "chapter_one = raw[5464:23725]\n",
        "chapter_one = re.sub('\\n',' ',chapter_one)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0f08861",
      "metadata": {
        "id": "b0f08861"
      },
      "source": [
        "Just as for segmenting sentences into words, we can segment texts into sentence using the re.split function. If you run the code below you will get a list of words.\n",
        "\n",
        "**Problem 2: What regular expression could we use to split a text into a list of sentences?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da8d3567",
      "metadata": {
        "id": "da8d3567"
      },
      "outputs": [],
      "source": [
        "to_split_on_sent = re.compile(\" \")\n",
        "C_and_P_sentences = to_split_on_sent.split(chapter_one)\n",
        "print(C_and_P_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For Loops\n",
        "\n",
        "A commonly used tool in programming is the \"for loop\". In its simplest form this allows us to iterate over (move through) a series of values. For example:"
      ],
      "metadata": {
        "id": "vhbvq9RwJpg3"
      },
      "id": "vhbvq9RwJpg3"
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0,10):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "et2uQKbmKMXC"
      },
      "execution_count": null,
      "outputs": [],
      "id": "et2uQKbmKMXC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can also be used to iterate through data structures:\n"
      ],
      "metadata": {
        "id": "qYmClKR9K00Q"
      },
      "id": "qYmClKR9K00Q"
    },
    {
      "cell_type": "code",
      "source": [
        "sentence=[\"the\",\"boy\",\"went\",\"to\",\"the\",\"park\"]\n",
        "for word in sentence:\n",
        "  print(word)"
      ],
      "metadata": {
        "id": "5lHU56r7K9mE"
      },
      "execution_count": null,
      "outputs": [],
      "id": "5lHU56r7K9mE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 3: How might we use a for loop plus what you learned above in order to turn an input text into list of sentences, where each sentence is a list of words?**"
      ],
      "metadata": {
        "id": "Zp8aQMfxPV-V"
      },
      "id": "Zp8aQMfxPV-V"
    },
    {
      "cell_type": "markdown",
      "id": "l1ijQKCx7HwX",
      "metadata": {
        "id": "l1ijQKCx7HwX"
      },
      "source": [
        "# Vector semantics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5BsrFJSs7IXp",
      "metadata": {
        "id": "5BsrFJSs7IXp"
      },
      "source": [
        "In this week's lecture you heard about Vector-based semantics. Today we will take a look at these models in Python.\n",
        "\n",
        "First we will use the method we arrived at above to segment and tokenize the whole of Crime and Punishment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VKgZMdQ_7UbA",
      "metadata": {
        "id": "VKgZMdQ_7UbA"
      },
      "outputs": [],
      "source": [
        "to_split_on_sent = re.compile(\"\\\\. \")\n",
        "to_split_on_words = re.compile(\" \\\\.|, | \")\n",
        "C_and_P_tokens_sentences=[]\n",
        "\n",
        "C_and_P_sentences = to_split_on_sent.split(raw)\n",
        "for sentence in C_and_P_sentences:\n",
        "    C_and_P_tokens_sentences.append(to_split_on_words.split(sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QRf6Fbn97WZl",
      "metadata": {
        "id": "QRf6Fbn97WZl"
      },
      "source": [
        "Next we will build a cooccurence matrix using the following code. The purpose of this is to aid your conceptual understanding by looking at the output, and you aren't expected to read or understand this code. Although if you come back to it later in the semester you may well be able to figure it out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "c_and_p=C_and_P_tokens_sentences\n",
        "c_and_p = [x for l in c_and_p for x in l]\n",
        "token_count = len(c_and_p)\n",
        "type_list = list(set(c_and_p))\n",
        "# The type count is the number of unique words. The token count is the total number of words including repetitions.\n",
        "type_count = len(type_list)\n",
        "# We create a matrix in which to store the counts for each word-by-word co-occurence\n",
        "M = np.zeros((type_count, type_count))\n",
        "window_size = 2\n",
        "\n",
        "for i, word in enumerate(c_and_p):\n",
        "            #print(str(i) + word)\n",
        "            # Find the index in the tokenized sentence vector for the beginning of the window (the current token minus window size or zero whichever is the lower)\n",
        "            begin = max(i - window_size, 0)\n",
        "            # Find the index in the tokenized sentence vector for the end of the window (the current token plus window size or the length of the sentence whichever is the lower)\n",
        "            end  = min(i + window_size, token_count)\n",
        "            # Extract the text from beginning of window to the end\n",
        "            context = c_and_p[begin: end + 1]\n",
        "            # Remove the target word from its own window\n",
        "            context.remove(c_and_p[i])\n",
        "            # Find the row for the current target word\n",
        "            current_row = type_list.index(c_and_p[i])\n",
        "            # Iterate over the window for this target word\n",
        "            for token in context:\n",
        "                # Find the ID and hence the column index for the current token\n",
        "                current_col = type_list.index(token)\n",
        "                # Add 1 to the current context word dimension for the current target word\n",
        "                M[current_row, current_col] += 1"
      ],
      "metadata": {
        "id": "iDK8ktJkrWh9"
      },
      "id": "iDK8ktJkrWh9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine(a,b):\n",
        "  return(np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b)))"
      ],
      "metadata": {
        "id": "HLZ0LnbUyBjH"
      },
      "id": "HLZ0LnbUyBjH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "w1 = \"walk\"\n",
        "w2 = \"run\"\n",
        "w3 = \"shine\"\n",
        "w1_index = type_list.index(w1)\n",
        "w2_index = type_list.index(w2)\n",
        "w3_index = type_list.index(w3)\n",
        "w1_vec=M[type_list.index(w1),]\n",
        "w2_vec=M[type_list.index(w2),]\n",
        "w3_vec=M[type_list.index(w3),]\n"
      ],
      "metadata": {
        "id": "ADR14jaUrvbF"
      },
      "id": "ADR14jaUrvbF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine(w1_vec,w2_vec)"
      ],
      "metadata": {
        "id": "r7OHB2XgwpW4"
      },
      "id": "r7OHB2XgwpW4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pretrained embeddings\n",
        "\n",
        "Vectors are best when learned from very large text collections. However learning such vectors, particular using neural network methods rather than simple counting, is very computationally intensive. As a result most people make use of pretrained embeddings such as those found at\n",
        "\n",
        "https://code.google.com/archive/p/word2vec/\n",
        "\n",
        "or\n",
        "\n",
        "https://nlp.stanford.edu/projects/glove/"
      ],
      "metadata": {
        "id": "G20YY35vjV53"
      },
      "id": "G20YY35vjV53"
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1Km0l6ekgFbpB5VS5TrxQDtyRon5eQVvL\n",
        "!unzip glove.6B.100d.txt.zip"
      ],
      "metadata": {
        "id": "EK7FNUuonAgN"
      },
      "id": "EK7FNUuonAgN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "embedding_file = 'glove.6B.100d.txt'\n",
        "#embedding_file = f.read()\n",
        "embeddings=[]\n",
        "type_list=[]\n",
        "with open(embedding_file) as fp:\n",
        "            for line in fp.readlines():\n",
        "                line = line.split(\" \")\n",
        "                word = line[0]\n",
        "                vec = np.array([float(x) for x in line[1:]])\n",
        "                type_list.append(word)\n",
        "                embeddings.append(vec)\n",
        "M=np.array((embeddings))"
      ],
      "metadata": {
        "id": "grtFYZDcfrvZ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "grtFYZDcfrvZ"
    },
    {
      "cell_type": "code",
      "source": [
        "w1 = \"football\"\n",
        "w2 = \"rugby\"\n",
        "w3 = \"cricket\"\n",
        "w1_index = type_list.index(w1)\n",
        "w2_index = type_list.index(w2)\n",
        "w3_index = type_list.index(w3)\n",
        "w1_vec=M[w1_index,]\n",
        "w2_vec=M[w2_index,]\n",
        "w3_vec=M[w3_index,]"
      ],
      "metadata": {
        "id": "P190dF8Kq3VW"
      },
      "execution_count": null,
      "outputs": [],
      "id": "P190dF8Kq3VW"
    },
    {
      "cell_type": "code",
      "source": [
        "cosine(w1_vec,w2_vec)"
      ],
      "metadata": {
        "id": "M2JGAiHt9Y76"
      },
      "id": "M2JGAiHt9Y76",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine(w1_vec,w3_vec)"
      ],
      "metadata": {
        "id": "EWE3Zx0M9ZSD"
      },
      "id": "EWE3Zx0M9ZSD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine(w2_vec,w3_vec)"
      ],
      "metadata": {
        "id": "fzhWBHyY9dte"
      },
      "id": "fzhWBHyY9dte",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 4. Calculate the cosine between the words above. What do the cosine values tell us?**"
      ],
      "metadata": {
        "id": "6WYSDJLF7h2l"
      },
      "id": "6WYSDJLF7h2l"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finding the most similar words\n",
        "\n",
        "One thing we often want to do is to find the most similar words to a given word/vector. An exhaustive N x N comparison is very time consuming, and so we can make use of an efficient \"nearest neighbours\" finding algorithm. We are just using this algorithm here so we won't go into it in any detail."
      ],
      "metadata": {
        "id": "1b6fOj5BTFjn"
      },
      "id": "1b6fOj5BTFjn"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(M)"
      ],
      "metadata": {
        "id": "gutnPC3Dv-KV"
      },
      "execution_count": null,
      "outputs": [],
      "id": "gutnPC3Dv-KV"
    },
    {
      "cell_type": "code",
      "source": [
        "w=\"football\"\n",
        "w_index = type_list.index(w)\n",
        "w_vec = M[w_index,]\n",
        "for i in nbrs.kneighbors([w_vec])[1][0]:\n",
        "  print(type_list[i])"
      ],
      "metadata": {
        "id": "kgcxDOU70gYO"
      },
      "execution_count": null,
      "outputs": [],
      "id": "kgcxDOU70gYO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 5. Find some examples where the system fails and explain why you think it has done so.**"
      ],
      "metadata": {
        "id": "144-ZuZ37R9L"
      },
      "id": "144-ZuZ37R9L"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analogical reasoning"
      ],
      "metadata": {
        "id": "3U1nSPzz6637"
      },
      "id": "3U1nSPzz6637"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I80Hq6i6sZZj"
      },
      "source": [
        "Another semantic property of embeddings is their ability to capture relational meanings. In an important early vector space model of cognition, Rumelhart and Abrahamson (1973) proposed the parallelogram model for solving simple analogy problems of the form a is to b as a* is to what?. In such problems, a system given a problem like apple:tree::grape:?, i.e., apple is to tree as  grape is to , and must fill in the word vine.\n",
        "\n",
        "In the parallelogram model, the vector from the word apple to the word tree (= tree âˆ’ apple) is added to the vector for grape (grape); the nearest word to that point is returned.\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "I80Hq6i6sZZj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rulVu7_dsZZj"
      },
      "outputs": [],
      "source": [
        "w1 = \"apple\"\n",
        "w2 = \"tree\"\n",
        "w3 = \"grape\"\n",
        "w1_index = type_list.index(w1)\n",
        "w2_index = type_list.index(w2)\n",
        "w3_index = type_list.index(w3)\n",
        "w1_vec = M[w1_index,]\n",
        "w2_vec = M[w2_index,]\n",
        "w3_vec = M[w3_index,]\n",
        "\n",
        "spatial_relationship = w2_vec - w1_vec\n",
        "w4_vec = w3_vec + spatial_relationship\n",
        "\n",
        "nbrs.kneighbors([w4_vec])\n",
        "for i in nbrs.kneighbors([w4_vec])[1][0]:\n",
        "  print(type_list[i])"
      ],
      "id": "rulVu7_dsZZj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 6: Come up with analogical reasoning problems of your own and use the code to solve it. Try to find one for which the approach works and one for which it doesn't work.**"
      ],
      "metadata": {
        "id": "cJ3PFSNh_XkH"
      },
      "id": "cJ3PFSNh_XkH"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}